# -*- coding: utf-8 -*-
"""final MJCN(Paper).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EkWAwGIDpj4iQYJFaofxPLVF2TeNpcOY
"""

import tensorflow as tf
from tensorflow.keras.layers import Layer, Conv2D, Flatten, Activation
from tensorflow.keras.models import Sequential

class MultiplicationLayer(Layer):
    def __init__(self, num_filters_mult, L1, L2, **kwargs):
        super(MultiplicationLayer, self).__init__(**kwargs)
        self.num_filters_mult = num_filters_mult
        self.L1 = L1
        self.L2 = L2

    def build(self, input_shape):
        self.n = input_shape[2]  # Get the 'n' dimension from the input shape
        self.v = self.add_weight(name='v',
                                 shape=(self.n, 5,self.L1 * self.L2),  # Update the shape to (n, 2, L2)
                                 initializer='random_normal',
                                 trainable=True)
        super(MultiplicationLayer, self).build(input_shape)

    def call(self, inputs):
        # Initialize an empty list to store results for each filter
        results = []

        # Reshape v to match the desired shape
        v_reshaped = tf.reshape(self.v, (self.n, 5, self.L1 * self.L2))

        # Iterate over the multiplication filters (L2)
        for i in range(self.L2):
            # Iterate over the input channels (L1)
            for j in range(self.L1):
                # Perform matrix multiplication between input and v
                channel_result = tf.matmul(inputs[:, :, :, j], v_reshaped[:, :, i * self.L1 + j])

                # Append the channel result to the results list
                results.append(channel_result)

        # Stack the results to form the final result tensor of size [m x 2 x( L1 x L2)]
        result = tf.stack(results, axis=-1)
        #The responses of this layer are then added to obtain a single feature matrix of size m x 2;.
        # Sum the responses along the last axis to obtain a single feature matrix of size m x 2
        result = tf.reduce_sum(result, axis=-1)


        return result


# Step 1: Create MJCN model
def create_mjcn_model(input_shape, num_filters_conv, num_filters_mult, L1, L2):
    model = Sequential()
    model.add(Conv2D(num_filters_conv, kernel_size=(5, 5), activation='relu', input_shape=input_shape, padding='same'))
    model.add(MultiplicationLayer(num_filters_mult, L1, L2))
    model.add(Activation('sigmoid'))
    model.add(Flatten())
    return model

# Example usage:
input_shape = (28, 28, 1)  # Adjust the input shape as needed
num_filters_conv = 8
num_filters_mult = 8
L1 = 8
L2 = 8
mjcn_model = create_mjcn_model(input_shape, num_filters_conv, num_filters_mult, L1, L2)
print(mjcn_model.summary())

from keras.src.callbacks import learning_rate_schedule
import numpy as np
import tensorflow as tf
from sklearn.metrics import accuracy_score
from sklearn.metrics import pairwise_distances
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
import random

def evaluate_classifier(predictions, true_labels):
    accuracy = accuracy_score(true_labels, predictions)
    return accuracy



"""start

"""







# Initialization
def initialize_population(pop_size, num_variables):
    return np.random.rand(pop_size, num_variables)  # Randomly initialize population





import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist

# Load the MNIST dataset
(train_images, train_labels),  (test_images, test_labels)  = mnist.load_data()

# Initialize variables to store the balanced dataset
balanced_images = []
balanced_labels = []
samples_per_class = 100  # Number of samples per digit class

for digit_class in range(10):
    # Filter images and labels for the current digit class
    digit_images = train_images[train_labels == digit_class]
    digit_labels = train_labels[train_labels == digit_class]

    # Randomly shuffle the samples for this digit class
    indices = np.arange(len(digit_labels))
    np.random.shuffle(indices)
    digit_images = digit_images[indices]
    digit_labels = digit_labels[indices]

    # Take the first 'samples_per_class' samples for this digit class
    balanced_images.append(digit_images[:samples_per_class])
    balanced_labels.append(digit_labels[:samples_per_class])

# Combine the balanced data for all digit classes
balanced_images = np.concatenate(balanced_images)
balanced_labels = np.concatenate(balanced_labels)

# Shuffle the balanced dataset
indices = np.arange(len(balanced_labels))
np.random.shuffle(indices)
balanced_images = balanced_images[indices]
balanced_labels = balanced_labels[indices]

# Optionally, normalize the pixel values to the range [0, 1]
balanced_images = balanced_images / 255.0

# Optionally, convert the labels to one-hot encoding
balanced_labels_one_hot = tf.keras.utils.to_categorical(balanced_labels, num_classes=10)

# Check the shape of the balanced dataset
print("Shape of balanced_images:", balanced_images.shape)
print("Shape of balanced_labels_one_hot:", balanced_labels_one_hot.shape)
num_classes = 10

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have a list 'balanced_labels' containing the labels of your balanced dataset
# For example, you can use the 'balanced_labels' from the previously provided code to create a balanced dataset

# Calculate the class distribution in the balanced dataset
unique_labels, class_counts = np.unique(balanced_labels, return_counts=True)

# Create a bar diagram to visualize the class distribution
plt.figure(figsize=(10, 6))
plt.bar(unique_labels, class_counts)
plt.xlabel('Class Label')
plt.ylabel('Count')
plt.title('Class Distribution in Balanced Dataset')
plt.xticks(unique_labels)  # Set x-axis ticks to class labels
plt.show()



best_weights= None
worst_weights= None
max_obj=0
max_fitness=float('-inf')
min_fitness= float('inf')



# Step 2: Get initial weights of the model
def get_initial_weights(model):
    return [layer.get_weights() for layer in model.layers]





def calculate_inter_class_distance(y_true, class_predictions):
    num_classes = y_true.shape[1]
    num_features = class_predictions.shape[1]
    inter_class_distance = 0.0

    for i in range(num_classes):
        for j in range(i + 1, num_classes):
            for d in range(num_features):
                class_i_indices = np.where(y_true[:, i] == 1)[0]
                class_j_indices = np.where(y_true[:, j] == 1)[0]

                if len(class_i_indices) > 0 and len(class_j_indices) > 0:
                    mean_i = np.mean(class_predictions[class_i_indices, d])
                    mean_j = np.mean(class_predictions[class_j_indices, d])
                    inter_class_distance += np.abs(mean_i - mean_j)

    return inter_class_distance

def calculate_intra_class_variance(y_true, class_predictions):
    num_classes = y_true.shape[1]
    num_features = class_predictions.shape[1]
    intra_class_variance = 0.0

    for i in range(num_classes):
        for d in range(num_features):
            class_i_indices = np.where(y_true[:, i] == 1)[0]

            if len(class_i_indices) > 0:
                mean_i = np.mean(class_predictions[class_i_indices, d])
                variance_i = np.var(class_predictions[class_i_indices, d])
                intra_class_variance += variance_i

    return intra_class_variance


# Step 4: Define the objective function to optimize weights based on inter-class distance and intra-class variance
def objective_function( y_true, y_pred, alpha, beta):
    inter_class_distance = calculate_inter_class_distance(y_true, y_pred)
    intra_class_variance = calculate_intra_class_variance(y_true, y_pred)
    # Calculate individual objectives for inter-class distance and intra-class variance
    individual_inter_class_distances = []
    individual_intra_class_variances = []
    for class_idx in range(num_classes):
      # Extract samples for the current class
      y_true_class = y_true[:, class_idx]
      y_pred_class = y_pred[:, class_idx]

      # Calculate the inter-class distance for the current class
      inter_class_distance_class = tf.reduce_mean(tf.abs(y_true_class - y_pred_class))

      # Calculate the intra-class variance for the current class
      intra_class_variance_class = tf.reduce_mean(tf.math.reduce_variance(y_true_class - y_pred_class, axis=0))

      # Append the calculated values to the respective lists
      individual_inter_class_distances.append(inter_class_distance_class)
      individual_intra_class_variances.append(intra_class_variance_class)
    ##########################################################



    # Calculate the sum of individual objectives for inter-class distance and intra-class variance
    sum_inter_class_distances = tf.reduce_sum(individual_inter_class_distances)
    sum_intra_class_variances = tf.reduce_sum(individual_intra_class_variances)

    # Calculate the updated multi-objective expression
    total_objective = (alpha * inter_class_distance / sum_inter_class_distances) - (beta * intra_class_variance / sum_intra_class_variances)

    return total_objective

def custom_loss(y_true, y_pred):
    # Calculate the inter-class distance and intra-class variance here based on y_true and y_pred
    # Combine the objectives using the multi-objective expression

    total_objective = objective_function(y_true, y_pred, alpha=0.2, beta=0.8)
    return total_objective

# Step 5: Use objective expression to find new weights for the model and set them to the model
def update_model_weights(model, train_images, train_labels,r1, r2):
    global best_weights, worst_weights, max_fitness,min_fitness
    # Create a copy of the model to avoid updating the original model
    updated_model = tf.keras.models.clone_model(model)
    weights=model.get_weights()
    current_predictions=model.predict(train_images)
    # Evaluate the total objective for the current weights
    current_total_objective = objective_function(train_labels,current_predictions,alpha=0.2, beta=0.8)
    if current_total_objective>max_fitness:
      max_fitness=current_total_objective
      best_weights= weights
    if current_total_objective<min_fitness:
      min_fitness=current_total_objective
      worst_weights= weights
    if best_weights is  None:
    # Initialize best_weights and worst_weights with the first set of weights
        print("initialize global weights")
        best_weights = weights
        worst_weights = weights
    # Update the weights using the formula: Pnew = Pi + r1 × (Pbest − |Pi|) − r2 × (Pworst − |Pi|)
    new_weights= []

    for i in range(len(weights)):
        r1= random.uniform(0.0, 1.0)
        r2= random.uniform(0.0, 1.0)
        new_weight = weights[i] + r1 * (best_weights[i] - np.abs(weights[i])) - r2 * (worst_weights[i] - np.abs(weights[i]))
        new_weights.append(new_weight)




    # Set the new weights to updated model
    updated_model.set_weights(new_weights)
    updated_predictions= updated_model.predict(train_images)


        # Calculate O(w, v) and add it to the list
    O_ = calculate_inter_class_distance(train_labels, current_predictions)
    O_i = calculate_inter_class_distance(train_labels, updated_predictions)
    # Calculate Q(w, v) and add it to the list
    Q_ = calculate_intra_class_variance(train_labels, current_predictions)
    Q_i = calculate_intra_class_variance(train_labels, updated_predictions)
    print("current--------- inter_class_distance",O_, "intra_class_variance",Q_)
    print("updated--------- inter_class_distance",O_i, "intra_class_variance",Q_i)
    if (((O_i > O_ ) and (Q_i < Q_)) or ((O_i == O_ ) and (Q_i < Q_)) or ((O_i > O_ ) and (Q_i == Q_) )):
      print("update model")
      #model = tf.keras.models.clone_model(updated_model)
      model=updated_model



    return model, model.get_weights(), objective_function(train_labels,model.predict(train_images),alpha=0.2, beta=0.8)


# Step 6: Repeat optimization process for a specified number of maximum iterations
def optimize_model(model, train_data,train_labels, max_iter, r1,r2):
    global best_weights, worst_weights
    for i in range(max_iter):
        print("epoch "+str(i+1)+"--------")
        model,updated_weights,objective = update_model_weights(model, train_data, train_labels,r1,r2)
        #current_weights = update_model_weights(model, current_weights, learning_rate, train_images, train_labels)

    return model,updated_weights,objective
# Step 7: Get features from the model
def get_features(model, data):
    return model.predict(data)


from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Initialize lists to store accuracies for each classifier
svm_accuracies = []
rf_accuracies = []
knn_accuracies = []
svm_classifier = SVC(kernel='rbf', C=1.0, gamma='scale')
# Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42)
knn_classifier = KNeighborsClassifier(n_neighbors=10)
populations=[]
for p in range(5):

    print(f"Population {p}")
    mjcn_model = create_mjcn_model(input_shape, num_filters_conv, num_filters_mult, L1, L2)
    max_iter = 50
    mjcn_model,updated_weights,objective = optimize_model(mjcn_model, balanced_images, balanced_labels_one_hot, max_iter, r1=0.5, r2=0.5)
    populations.append([updated_weights,objective])
    train_features = get_features(mjcn_model, balanced_images)

    # Classify with SVM

    svm_classifier.fit(train_features, balanced_labels)
    svm_predicted_labels = svm_classifier.predict(train_features)
    svm_accuracy = accuracy_score(balanced_labels, svm_predicted_labels)
    svm_accuracies.append(svm_accuracy)

    # Classify with Random Forest

    rf_classifier.fit(train_features, balanced_labels)
    rf_predicted_labels = rf_classifier.predict(train_features)
    rf_accuracy = accuracy_score(balanced_labels, rf_predicted_labels)
    rf_accuracies.append(rf_accuracy)

    # Classify with KNN

    knn_classifier.fit(train_features, balanced_labels)
    knn_predicted_labels = knn_classifier.predict(train_features)
    knn_accuracy = accuracy_score(balanced_labels, knn_predicted_labels)
    knn_accuracies.append(knn_accuracy)

    print(f"Accuracy (SVM): {svm_accuracy:.2f}")
    print(f"Accuracy (Random Forest): {rf_accuracy:.2f}")
    print(f"Accuracy (KNN): {knn_accuracy:.2f}")

# Sort populations by objective values (fitness)
populations.sort(key=lambda x: x[1], reverse=True)

# Get the best population (with the highest fitness)
best_population = populations[0]

# Extract the best weights and objective value
best_weights = best_population[0]
best_objective = best_population[1]
print(best_weights)

# Print the best objective value
print(f"Best Objective Value: {best_objective}")

# Now, you can use the best_weights to predict features and calculate accuracy if needed.
# For example, to predict features and calculate accuracy using the best_weights:
best_model = create_mjcn_model(input_shape, num_filters_conv, num_filters_mult, L1, L2)
best_model.set_weights(best_weights)




train_features = get_features(best_model, balanced_images)

# Classify with SVM
#svm_classifier = SVC()
svm_classifier.fit(train_features, balanced_labels)
svm_predicted_labels = svm_classifier.predict(train_features)
svm_accuracy = accuracy_score(balanced_labels, svm_predicted_labels)


# Classify with Random Forest
#rf_classifier = RandomForestClassifier()
rf_classifier.fit(train_features, balanced_labels)
rf_predicted_labels = rf_classifier.predict(train_features)
rf_accuracy = accuracy_score(balanced_labels, rf_predicted_labels)


# Classify with KNN
#knn_classifier = KNeighborsClassifier(n_neighbors=5)
knn_classifier.fit(train_features, balanced_labels)
knn_predicted_labels = knn_classifier.predict(train_features)
knn_accuracy = accuracy_score(balanced_labels, knn_predicted_labels)


print(f"Accuracy (SVM): {svm_accuracy:.2f}")
print(f"Accuracy (Random Forest): {rf_accuracy:.2f}")
print(f"Accuracy (KNN): {knn_accuracy:.2f}")

import matplotlib.pyplot as plt



# Generate a list of iteration numbers (assuming one accuracy value per iteration)
iterations = range(1, len(svm_accuracies) + 1)

# Plot SVM accuracies in blue
plt.plot(iterations, svm_accuracies, marker='o', label='SVM', color='blue')

# Plot Random Forest accuracies in green
plt.plot(iterations, rf_accuracies, marker='o', label='Random Forest', color='green')

# Plot KNN accuracies in red
plt.plot(iterations, knn_accuracies, marker='o', label='KNN', color='red')

# Add labels and a legend
plt.xlabel('Iteration')
plt.ylabel('Accuracy')
plt.title('Classifier Accuracies Over Iterations')
plt.legend()

# Show the plot
plt.grid(True)
plt.show()

train_features[106]

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Initialize lists to store accuracies for each classifier
svm_accuracies = []
rf_accuracies = []
knn_accuracies = []
svm_classifier = SVC(kernel='rbf', C=1.0, gamma='scale')
# Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42)
knn_classifier = KNeighborsClassifier(n_neighbors=10)
for p in range(20):
    mjcn_model = create_mjcn_model(input_shape, num_filters_conv, num_filters_mult, L1, L2)
    print(f"Population {p}")

    max_iter = 100
    mjcn_model = optimize_model(mjcn_model, balanced_images, balanced_labels_one_hot, max_iter, r1=0.5, r2=0.5)

    train_features = get_features(mjcn_model, balanced_images)

    # Classify with SVM

    svm_classifier.fit(train_features, balanced_labels)
    svm_predicted_labels = svm_classifier.predict(train_features)
    svm_accuracy = accuracy_score(balanced_labels, svm_predicted_labels)
    svm_accuracies.append(svm_accuracy)

    # Classify with Random Forest

    rf_classifier.fit(train_features, balanced_labels)
    rf_predicted_labels = rf_classifier.predict(train_features)
    rf_accuracy = accuracy_score(balanced_labels, rf_predicted_labels)
    rf_accuracies.append(rf_accuracy)

    # Classify with KNN

    knn_classifier.fit(train_features, balanced_labels)
    knn_predicted_labels = knn_classifier.predict(train_features)
    knn_accuracy = accuracy_score(balanced_labels, knn_predicted_labels)
    knn_accuracies.append(knn_accuracy)

    print(f"Accuracy (SVM): {svm_accuracy:.2f}")
    print(f"Accuracy (Random Forest): {rf_accuracy:.2f}")
    print(f"Accuracy (KNN): {knn_accuracy:.2f}")

import matplotlib.pyplot as plt



# Generate a list of iteration numbers (assuming one accuracy value per iteration)
iterations = range(1, len(svm_accuracies) + 1)

# Plot SVM accuracies in blue
plt.plot(iterations, svm_accuracies, marker='o', label='SVM', color='blue')

# Plot Random Forest accuracies in green
plt.plot(iterations, rf_accuracies, marker='o', label='Random Forest', color='green')

# Plot KNN accuracies in red
plt.plot(iterations, knn_accuracies, marker='o', label='KNN', color='red')

# Add labels and a legend
plt.xlabel('Iteration')
plt.ylabel('Accuracy')
plt.title('Classifier Accuracies Over Iterations')
plt.legend()

# Show the plot
plt.grid(True)
plt.show()